{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a31557e4",
   "metadata": {},
   "source": [
    "# CS172 Assignment 1: CAPTCHAs\n",
    "Shanghaitech University 2025 Fall Computer Vision I\n",
    "\n",
    "Welcome to the first assignment of CS172. In this homework, we aim to build and train a neural network model to recognize CAPTCHAs consisting only of Arabic numerals (0-9). The primary focus is on generating synthetic verification code data, preprocessing it, and building a convolutional neural network (CNN) for character recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb446811",
   "metadata": {},
   "source": [
    "## Part 0: Setup\n",
    "Before running the notebook, ensure that your environment is properly set up. Install the necessary dependencies, which include `torch`, `torchvision`, `PIL`, and `matplotlib`. These can be installed via conda and pip using the command:\n",
    "```\n",
    "conda create -n 172a1 python=3.8\n",
    "conda activate 172a1\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "Then you can run the following code to load the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c68175ca-fa3d-4a93-be0b-85ca5ac7942b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchmetrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcs172\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ImageDataset\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcs172\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnetworks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_model\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcs172\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train, test\n\u001b[0;32m      8\u001b[0m utils\u001b[38;5;241m.\u001b[39mset_seed(\u001b[38;5;241m172\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\cs\\CV\\作业\\CS172-Assignment1\\cs172\\pipeline.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnotebook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MetricCollection\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcs172\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ImageAccuracy, DigitAccuracy\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m(model, device, dataloader, lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m, weight_decay \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m, num_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchmetrics'"
     ]
    }
   ],
   "source": [
    "import torch, os\n",
    "from cs172 import utils\n",
    "from cs172 import generate\n",
    "from cs172.datasets import ImageDataset\n",
    "from cs172.networks import get_model\n",
    "from cs172.pipeline import train, test\n",
    "\n",
    "utils.set_seed(172)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c99a70",
   "metadata": {},
   "source": [
    "## Part 1: Data Generation\n",
    "Implement the `generate_verification` function in `cs172/generate.py`, where we generate synthetic images of verification codes. Each image contains a series of Arabic numerals. \n",
    "You can modify parameters such as colors, noise level and noise type.\n",
    "\n",
    "After implementing this function, you can display some data by running the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942264d5-bc7e-4854-952c-62b73d44678a",
   "metadata": {},
   "outputs": [],
   "source": [
    "need_rotate = True # Do not modify this line\n",
    "images = [generate.generate_verification(need_rotate=need_rotate)[0] for _ in range(16)]\n",
    "display(utils.merge_images(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035433b4",
   "metadata": {},
   "source": [
    "To generate the training dataset, run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2626cc-7c8b-4705-bf5d-10787df4651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "need_rotate = True # if you do not want to rotate the image for training, feel free to set it to False\n",
    "image_size = (260, 80)\n",
    "data_num = 100000 # total data num\n",
    "save_folder = 'data' # set data folder\n",
    "\n",
    "# You can comment out the line to avoid duplicate data generation\n",
    "generate.save_certification_data(image_size, data_num, save_folder, need_rotate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f613b25a",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "- What are some real-world challenges you might encounter when working with CAPTCHA images compared to our synthetic ones (e.g., distortions, noise)?\n",
    "- Would using different fonts or noise levels impact the model's learning ability? How would you test this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354d5a4c",
   "metadata": {},
   "source": [
    "## Part 2: Data Preprossing\n",
    "Implement `__len__` and `__getitem__` functions in `cs172/datasets.py` for a customized `Dataset` class.\n",
    "\n",
    "Then you can run the following code to preprocess the generated images to fit the neural network's input and batch the data using Pytorch's `DataLoader`. \n",
    "\n",
    "You can change the `transform` variable which may include steps such as resizing the images, normalizing pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d964cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can try different transform for data augmentation\n",
    "# torchvision.transforms may be useful\n",
    "transform = None\n",
    "\n",
    "save_folder = 'data'\n",
    "dataset = ImageDataset(imgdir=save_folder, transform=transform)\n",
    "\n",
    "print(len(dataset))\n",
    "\n",
    "ratio = 0.8\n",
    "train_size = int(ratio * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# you can modify batch_size if 'CUDA OUT OF MEMORY'\n",
    "batch_size = 128\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size,\n",
    "    shuffle=True, num_workers=0)\n",
    "valid_dataloader = torch.utils.data.DataLoader(\n",
    "    valid_dataset, batch_size=batch_size,\n",
    "    shuffle=False, num_workers=0)\n",
    "\n",
    "img, label = next(iter(train_dataloader))\n",
    "print(img[0].min(), img[0].max())\n",
    "print(label[0])\n",
    "img.shape, label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9516f0",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "- How important is data normalization in neural networks? What could happen if you skip this step?\n",
    "- What effect does batch size have on model performance and training speed? How would you balance memory usage with batch size selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56b9814",
   "metadata": {},
   "source": [
    "## Part 3: Model Architecture\n",
    "Implement ResNet18 in `networks.py`, which contains `SimpleResBlock` and `ResNet18` class. \n",
    "\n",
    "For `SimpleResBlock`, you can follow this structure to implement each layer(weight layer, activation funcyion, residual...)\n",
    "\n",
    "The residual block has two $3 \\times 3$ convolutional layers with the same number of output channels. Each convolutional layer is followed by a batch normalization layer and a ReLU activation function. \n",
    "Then, we skip these two convolution operations and add the input directly before the final ReLU activation function. \n",
    "This kind of design requires that the output of the two convolutional layers has to be of the same shape as the input, so that they can be added together. \n",
    "If we want to change the number of channels, we need to introduce an additional $1 \\times 1$ convolutional layer to transform the input into the desired shape for the addition operation. \n",
    "\n",
    "![ResBlock](assets/resnet-block.svg \"picture is from https://d2l.ai/\")\n",
    "\n",
    "For `ResNet18`, you can follow this structure:\n",
    "\n",
    "The first two layers of ResNet are the same as those of the GoogLeNet we described before: the $7 \\times 7$ convolutional layer with 64 output channels and a stride of 2 is followed by the $3 \\times 3$ max-pooling layer with a stride of 2. The difference is the batch normalization layer added after each convolutional layer in ResNet.\n",
    "\n",
    "ResNet uses four modules made up of residual blocks, each of which uses several residual blocks with the same number of output channels. The number of channels in the first module is the same as the number of input channels. Since a max-pooling layer with a stride of 2 has already been used, it is not necessary to reduce the height and width. In the first residual block for each of the subsequent modules, the number of channels is doubled compared with that of the previous module, and the height and width are halved.\n",
    "\n",
    "Then, we add all the modules to ResNet. Here, two residual blocks are used for each module. Lastly, we add a global average pooling layer, followed by the fully connected layer output.\n",
    "\n",
    "There are four convolutional layers in each module (excluding the $1 \\times 1$ convolutional layer). Together with the first $7 \\times 7$ convolutional layer and the final fully connected layer, there are 18 layers in total. Therefore, this model is commonly known as ResNet-18. \n",
    "\n",
    "![CAPTCHAs](assets/resnet18-90.svg \"picture is from https://d2l.ai/, which is a very useful deep learning book\") \n",
    "\n",
    "Pictures and descriptions come from [Dive into Deep Learning](https://d2l.ai/), which is a very helpful deep learning book.\n",
    "\n",
    "After implementing the networks architecture, you can run the following code to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d540247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change model_name to compare with the ResNet implementation in torchvision\n",
    "model_name = \"myresnet18\"\n",
    "assert model_name in (\"myresnet18\", \"resnet18\", \"resnet34\")\n",
    "model = get_model(model_name)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22cabea",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "- What modifications could you make to the current architecture if the model struggles with overfitting or underfitting?\n",
    "- Would you consider using a different type of model (like MLP) for this task? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0f37b5",
   "metadata": {},
   "source": [
    "## Part 4: Training\n",
    "In`pipeline.py`, implement a training loop for the model. You will fine-tune hyperparameters like learning rate, batch size, and optimizer, while monitoring the model's performance with loss and accuracy metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7505dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if os.path.exists(\"model_weights.pth\"):\n",
    "    model_weights = torch.load(\"model_weights.pth\")\n",
    "    model.load_state_dict(model_weights)\n",
    "else:\n",
    "    # you can try different hyperparameters by passing args `lr`, `weight_decay`, `num_epoch`\n",
    "    train(model, device, train_dataloader)\n",
    "\n",
    "# save model weights, thus you can try different hyperparameters without retraining\n",
    "torch.save(model.state_dict(), \"model_weights.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1075f1",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "- How would you approach adjusting the learning rate if the model's accuracy plateaus early? Could a learning rate schedule improve training?\n",
    "- Would regularization methods (such as dropout or L2 regularization) be beneficial for this task? When would you apply them?\n",
    "- How would you know when to stop training the model? Should early stopping be implemented, and if so, how would you determine when?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08322285",
   "metadata": {},
   "source": [
    "## Part 5: Evaluation\n",
    "After training, evaluate the model's performance on the validation and test datasets using metrics such as digit accuracy and image accuracy. Visualize predictions and errors to understand the model’s strengths and weaknesses.\n",
    "You could add more experiments and disscussion (like loss decay, metric table, hyperparameter ablation study) in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080deaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on your valid data\n",
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "metrics = test(model, device, valid_dataloader)\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}:\", value.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123fa10f",
   "metadata": {},
   "source": [
    "Experiment with various hyperparameters — including learning rates, batch sizes, network architecture (e.g., number of layers, filter sizes, and activations), loss decay, and metric settings — on your own generated validation set. Analyze the results and report the best-performing configuration here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ada45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on sample test data\n",
    "# There are 5 test sets in 'samples'. To receive full points for each test set,\n",
    "# your model must achieve a digit accuracy ≥ 0.95 and an image accuracy ≥ 0.75.\n",
    "folders = sorted(os.listdir('samples'))\n",
    "for folder in folders:\n",
    "    if folder == \"test_alphabet\":\n",
    "        continue \n",
    "\n",
    "    test_dataset = ImageDataset(os.path.join('samples', folder), transform=transform)\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=batch_size,\n",
    "        shuffle=False, num_workers=0,\n",
    "    )\n",
    "    metrics = test(model, device, test_dataloader)\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key} for {folder}:\", value.item())\n",
    "    \n",
    "    model.eval()\n",
    "    images, imgs, labels = zip(*[test_dataset.get_samples(idx) for idx in range(4)])\n",
    "    labels = [label.reshape(5, 10).argmax(axis=-1).tolist() for label in labels]\n",
    "    with torch.no_grad():\n",
    "        preds = [model(torch.tensor(img).unsqueeze(0).to(device))[0].reshape(5, 10).argmax(axis=-1).cpu().numpy().tolist() for img in imgs]\n",
    "    display(utils.merge_images(images, grid_size=(4, 1)))\n",
    "    print(labels)\n",
    "    print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08a31d8",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "- How reliable are accuracy metrics for evaluating the model? Would other metrics offer additional insights?\n",
    "- What kinds of errors are the most common—misrecognition of certain digits, complete failures? How could you address these types of errors?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358e36a1",
   "metadata": {},
   "source": [
    "## Part 6: Discussion\n",
    "There are 11 discussion questions above. You need to answer all of them, each worth 1 point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199d79b4",
   "metadata": {},
   "source": [
    "## Bonus: Alphabet CAPTCHA Recognition\n",
    "\n",
    "In this part, you are encouraged to extend your model to recognize alphabetic CAPTCHAs (A–Z, a–z).\n",
    "We have provided some pre-generated alphabet CAPTCHA images for test.\n",
    "\n",
    "To complete this part, you should:\n",
    "- Show several examples of the training data you generated or used.\n",
    "- Report the modifications you made compared to the numeric CAPTCHA setting (e.g., data generation, model architecture, or training strategy).\n",
    "- Report your training accuracy.\n",
    "- Report your accuracy on the provided test set.\n",
    "\n",
    "You can add new cells below to organize your experiments, show results, and summarize your findings.\n",
    "Feel free to explore different ideas — creativity is encouraged!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00304d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "transform = None\n",
    "batch_size = 128\n",
    "folder = 'test_alphabet'\n",
    "\n",
    "test_dataset = ImageDataset(os.path.join('samples', folder), transform=transform)\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size,\n",
    "    shuffle=False, num_workers=0,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS182",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
